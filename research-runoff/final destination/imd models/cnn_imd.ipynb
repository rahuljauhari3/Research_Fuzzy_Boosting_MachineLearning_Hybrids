{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd \n",
    "from keras.layers import LSTM,Dropout,Dense\n",
    "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/Users/rahuljauhari/Desktop/research runoff/final destination/merged_imd.csv\")\n",
    "df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "df.set_index('DateTime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly_mean = df\n",
    "monthly_mean = df.resample('M').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actual value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual=pd.read_excel(\"/Users/rahuljauhari/Desktop/research runoff/Calibrated and Validated.xlsx\") #monthly\n",
    "# df_actual=pd.read_excel(\"/Users/rahuljauhari/Desktop/research runoff/Historical_Scenario_1982_2020_and_Future_Runoff_Predictions_From_2021-2099_using_XGBoost_and_EFUSE_Models (2).xlsx\") #daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly\n",
    "observed_runnoff=df_actual['observed']\n",
    "\n",
    "# daily\n",
    "# observed_runnoff=df_actual.iloc[:,-1]\n",
    "# observed_runnoff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "def func(name):\n",
    "    x=0\n",
    "    y=0\n",
    "    inv= 0\n",
    "    if name=='zscore':\n",
    "        x_norm = zscore(monthly_mean)\n",
    "        y_norm = zscore(observed_runnoff)\n",
    "        # x_norm[x_norm > 3] = 2.8\n",
    "        # x_norm[x_norm < -3] = -2.8\n",
    "        # y_norm[y_norm >3] = 2.8\n",
    "        # y_norm[y_norm < -3] = -2.8\n",
    "        x=x_norm\n",
    "        y=y_norm\n",
    "    if name=='StandardScaler':\n",
    "        scaler = StandardScaler()\n",
    "        x_scaled = scaler.fit_transform(monthly_mean)\n",
    "        y_scaled = scaler.fit_transform(observed_runnoff.values.reshape(-1,1))\n",
    "        x_scaled[x_scaled > 3] = 2.8\n",
    "        x_scaled[x_scaled < -3] = -2.8\n",
    "        y_scaled[y_scaled >3] = 2.8\n",
    "        y_scaled[y_scaled < -3] = -2.8\n",
    "        x=      x_scaled  \n",
    "        y=y_scaled\n",
    "        inv = scaler\n",
    "        \n",
    "    if name == 'MinMaxScaler':\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        x_scaled = scaler.fit_transform(monthly_mean)\n",
    "        y_scaled = scaler.fit_transform(observed_runnoff.values.reshape(-1,1))\n",
    "        x=      x_scaled  \n",
    "        y=y_scaled\n",
    "        inv = scaler\n",
    "    return x,y,inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def rmse1(yt, yp): #lower the better\n",
    "    return np.sqrt(mean_squared_error(yt, yp))\n",
    "# Kling-Gupta effciency\n",
    "def kge1(yt, yp): #highqer the better\n",
    "    r = np.corrcoef(yt, yp,rowvar=False)[0, 1]\n",
    "    alpha = np.std(yp) / np.std(yt)\n",
    "    beta = np.mean(yp) / np.mean(yt)\n",
    "    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "    # r squared\n",
    "def r21(yt, yp): #higher the better\n",
    "    return 1 - np.sum((yt - yp)**2) / np.sum((yt - np.mean(yt))**2)\n",
    "    # Nash-Sutcliffe efficiency\n",
    "def nse(predictions, targets):\n",
    "    return (1-(np.sum((predictions-targets)**2)/np.sum((targets-np.mean(targets))**2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "x,y,inv = func('MinMaxScaler')\n",
    "x_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,shuffle=False)\n",
    "# validation_data 10% of train data\n",
    "x_train, x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kerastuner\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from keras import regularizers\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    for i in range(hp.Int('n_layers', 1, 4)):\n",
    "        model.add(Conv1D(filters=hp.Int('input_units', min_value=32, max_value=512, step=32), kernel_size=hp.Int('kernel_size', min_value=1, max_value=5, step=1), activation=hp.Choice('act_' + str(i), values=['relu', 'sigmoid','linear','tanh']), input_shape=(x_train.shape[1],1)))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(hp.Float('dropout_1', 0, 0.5, step=0.1, default=0.2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(layers.Dense(1,activation=hp.Choice('act_' + str(i), values=['relu', 'sigmoid','linear','tanh']),kernel_regularizer=regularizers.l2(hp.Float('l2', 0, 0.5, step=0.1, default=0.2))))\n",
    "    hp_lr = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    hp_optimizer = hp.Choice('optimizer', values=['sgd', 'rmsprop', 'adam'])\n",
    "\n",
    "    if hp_optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=hp_lr)\n",
    "    elif hp_optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=hp_lr)\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=hp_lr)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch( build_model, objective='mse', max_trials=10, executions_per_trial=1, directory='project', project_name='CNN')\n",
    "tuner.search(x_train, y_train, epochs=100,verbose=0,validation_data=(x_val, y_val))\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(\"train \",r2_score(y_train, best_model.predict(x_train)))\n",
    "print(\"val \",r2_score(y_val, best_model.predict(x_val)))\n",
    "print(\"test \",r2_score(y_test, best_model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()\n",
    "# optimizer used in best model\n",
    "print(best_model.optimizer.get_config())\n",
    "# activation used in best model\n",
    "best_model.layers[0].get_config()['activation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout used in best model\n",
    "best_model.layers[4].get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learnign rate\n",
    "best_model.optimizer.get_config()['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regulari\n",
    "best_model.layers[6].get_config()['kernel_regularizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "best_model.optimizer.get_config()['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = [ 'linear']\n",
    "optimizer = ['rmsprop']\n",
    "preprocess = ['MinMaxScaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "for act in activation:\n",
    "    for opt in optimizer:\n",
    "        for pre in preprocess:\n",
    "            model = Sequential()\n",
    "            x,y,inv_scaler= func(pre)\n",
    "            x_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,shuffle=False)\n",
    "            model.add(Conv1D(filters=64, kernel_size=5, activation=act, input_shape=(x_train.shape[1],1),kernel_initializer='he_normal'))\n",
    "            model.add(MaxPooling1D(pool_size=2))\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(256, activation=act))\n",
    "            model.add(Conv1D(filters=256, kernel_size=5, activation=act,kernel_initializer='he_normal'))\n",
    "            model.add(MaxPooling1D(pool_size=2))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(256, activation=act))\n",
    "            model.add(Dense(1, activation=act))\n",
    "            model.compile(optimizer=opt, loss='mean_squared_error', metrics=['accuracy'])\n",
    "            model.fit(x_train, y_train, epochs=100, batch_size=30, verbose=0)\n",
    "            y_pred = model.predict(x_test)\n",
    "            y_pred_train = model.predict(x_train)\n",
    "            try:\n",
    "                # _ = pd.DataFrame({'pre':pre,'act':act,'opt':opt,'r2_train':r2_score(y_train,y_pred_train),'r2_test':r2_score(y_test,y_pred)},index=[0])\n",
    "                # # _ = pd.DataFrame({'pre':pre,'act':act,'opt':opt,'rmse_train':rmse1(y_train,y_pred_train),'rmse_test':rmse1(y_test,y_pred),'kge_train':kge1(y_train,y_pred_train),'kge_test':kge1(y_test,y_pred),'r2_train':r21(y_train,y_pred_train),'r2_test':r21(y_test,y_pred)},index=[0])\n",
    "                # _.to_csv('/Users/rahuljauhari/Desktop/research runoff/results1/imd_cnn.csv',mode='a',header=True)\n",
    "                y= np.concatenate((y_pred_train,y_pred),axis=0)\n",
    "                # inverse transform\n",
    "                y_inv = inv_scaler.inverse_transform(y)\n",
    "                # to csv\n",
    "                pd.DataFrame(y_inv).to_csv('/Users/rahuljauhari/Desktop/research runoff/results1/imd_cnn_0.3.csv',mode='a',header=True)\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a19adc57dc9b86607f700ef6ca47dcfa3c63db12d19ee8d5249422558c9c076"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
